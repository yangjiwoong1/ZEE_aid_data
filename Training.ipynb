{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training AID dataset by SRCNN\n",
    "\n",
    "- reference code: https://github.com/worldstrat/worldstrat.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from src.train import *\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_train_command = [\n",
    "    # Batch size, gpus, limits\n",
    "    \"train.py\",\n",
    "    \"--batch_size\", \"1\", \n",
    "    \"--accelerator\", \"cpu\",\n",
    "    \"--devices\", \"1\",\n",
    "    \"--max_steps\", \"100\",\n",
    "    \"--precision\", \"16\",\n",
    "    \"--num_workers\", \"0\",\n",
    "\n",
    "    # Model/Hyperparameters\n",
    "    \"--model\", \"SRCNN\", # must be uppercase\n",
    "    \"--w_mse\", \"0.3\",\n",
    "    \"--w_mae\", \"0.4\",\n",
    "    \"--w_ssim\", \"0.3\",\n",
    "    \"--hidden_channels\", \"32\",\n",
    "    \"--residual_layers\", \"1\",\n",
    "    \"--padding_mode\", \"reflect\",\n",
    "    \"--sr_kernel_size\", \"1\",\n",
    "    \"--use_dropout\", \"False\",\n",
    "    \"--use_batchnorm\", \"False\",\n",
    "    \"--learning_rate\", \"1e-4\",\n",
    "\n",
    "    # Data\n",
    "    \"--root\", \"AID-dataset/\",\n",
    "    \"--zoom_factor\", \"2\", # 모델에 영향\n",
    "    \"--output_size\", \"300\", \"300\",\n",
    "    \"--chip_size\", \"300\", \"300\",\n",
    "    \"--chip_stride\", \"300\", \"300\",\n",
    "    \"--randomly_rotate_and_flip_images\", \"True\",\n",
    "    \"--shuffle\", \"True\",\n",
    "    \"--subset_train\", \"0.6\",\n",
    "\n",
    "    # logging\n",
    "    \"--use_wandb\",\n",
    "    # \"--benchmark_logging\", # require enable wandb\n",
    "\n",
    "    \"--upload_checkpoint\",\n",
    "\n",
    "    # early stopping\n",
    "    \"--early_stop\",\n",
    "    \"--early_stop_patience\", \"8\",\n",
    "    \"--early_stop_min_delta\", \"0.001\",\n",
    "]\n",
    "\n",
    "def run_training_command(training_command, running_on_windows=True):\n",
    "    sys.argv = training_command\n",
    "    if running_on_windows:\n",
    "        sys.argv += [\"--num_workers\", \"0\"] # 윈도우에서는 멀티프로세싱 비효율(than linux)\n",
    "    cli_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_lr dataset path: AID-dataset/train/LR_x4\n",
      "train_hr dataset path: AID-dataset/train/HR\n",
      "val_lr dataset path: AID-dataset/val/LR_x4\n",
      "val_hr dataset path: AID-dataset/val/HR\n",
      "test_lr dataset path: AID-dataset/test/LR_x4\n",
      "test_hr dataset path: AID-dataset/test/HR\n",
      "Shuffling the dataset splits using 42\n",
      "Shuffling the dataset splits using 42\n",
      "Shuffling the dataset splits using 42\n",
      "Shuffling the dataset splits using 42\n",
      "Train set size: 4710\n",
      "Val set size: 150\n",
      "Test set size: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:263: UserWarning: Attribute 'backbone' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['backbone'])`.\n",
      "  rank_zero_warn(\n",
      "/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:736: UserWarning: You passed `Trainer(accelerator='cpu', precision=16)` but native AMP is not supported on CPU. Using `precision='bf16'` instead.\n",
      "  rank_zero_warn(\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                                         | Type                    | Params\n",
      "------------------------------------------------------------------------------------------\n",
      "0  | backbone                                     | SRCNN                   | 334 K \n",
      "1  | backbone.encoder                             | DoubleConv2d            | 38.9 K\n",
      "2  | backbone.encoder.doubleconv2d                | Sequential              | 38.9 K\n",
      "3  | backbone.encoder.doubleconv2d.0              | Conv2d                  | 1.7 K \n",
      "4  | backbone.encoder.doubleconv2d.1              | BatchNorm2d             | 128   \n",
      "5  | backbone.encoder.doubleconv2d.2              | PReLU                   | 1     \n",
      "6  | backbone.encoder.doubleconv2d.3              | Dropout                 | 0     \n",
      "7  | backbone.encoder.doubleconv2d.4              | Conv2d                  | 36.9 K\n",
      "8  | backbone.encoder.doubleconv2d.5              | BatchNorm2d             | 128   \n",
      "9  | backbone.encoder.doubleconv2d.6              | PReLU                   | 1     \n",
      "10 | backbone.encoder.doubleconv2d.7              | Dropout                 | 0     \n",
      "11 | backbone.body                                | Sequential              | 295 K \n",
      "12 | backbone.body.0                              | ResidualBlock           | 74.0 K\n",
      "13 | backbone.body.0.residualblock                | DoubleConv2d            | 74.0 K\n",
      "14 | backbone.body.0.residualblock.doubleconv2d   | Sequential              | 74.0 K\n",
      "15 | backbone.body.0.residualblock.doubleconv2d.0 | Conv2d                  | 36.9 K\n",
      "16 | backbone.body.0.residualblock.doubleconv2d.1 | BatchNorm2d             | 128   \n",
      "17 | backbone.body.0.residualblock.doubleconv2d.2 | PReLU                   | 1     \n",
      "18 | backbone.body.0.residualblock.doubleconv2d.3 | Dropout                 | 0     \n",
      "19 | backbone.body.0.residualblock.doubleconv2d.4 | Conv2d                  | 36.9 K\n",
      "20 | backbone.body.0.residualblock.doubleconv2d.5 | BatchNorm2d             | 128   \n",
      "21 | backbone.body.0.residualblock.doubleconv2d.6 | PReLU                   | 1     \n",
      "22 | backbone.body.0.residualblock.doubleconv2d.7 | Dropout                 | 0     \n",
      "23 | backbone.body.1                              | ResidualBlock           | 74.0 K\n",
      "24 | backbone.body.1.residualblock                | DoubleConv2d            | 74.0 K\n",
      "25 | backbone.body.1.residualblock.doubleconv2d   | Sequential              | 74.0 K\n",
      "26 | backbone.body.1.residualblock.doubleconv2d.0 | Conv2d                  | 36.9 K\n",
      "27 | backbone.body.1.residualblock.doubleconv2d.1 | BatchNorm2d             | 128   \n",
      "28 | backbone.body.1.residualblock.doubleconv2d.2 | PReLU                   | 1     \n",
      "29 | backbone.body.1.residualblock.doubleconv2d.3 | Dropout                 | 0     \n",
      "30 | backbone.body.1.residualblock.doubleconv2d.4 | Conv2d                  | 36.9 K\n",
      "31 | backbone.body.1.residualblock.doubleconv2d.5 | BatchNorm2d             | 128   \n",
      "32 | backbone.body.1.residualblock.doubleconv2d.6 | PReLU                   | 1     \n",
      "33 | backbone.body.1.residualblock.doubleconv2d.7 | Dropout                 | 0     \n",
      "34 | backbone.body.2                              | ResidualBlock           | 74.0 K\n",
      "35 | backbone.body.2.residualblock                | DoubleConv2d            | 74.0 K\n",
      "36 | backbone.body.2.residualblock.doubleconv2d   | Sequential              | 74.0 K\n",
      "37 | backbone.body.2.residualblock.doubleconv2d.0 | Conv2d                  | 36.9 K\n",
      "38 | backbone.body.2.residualblock.doubleconv2d.1 | BatchNorm2d             | 128   \n",
      "39 | backbone.body.2.residualblock.doubleconv2d.2 | PReLU                   | 1     \n",
      "40 | backbone.body.2.residualblock.doubleconv2d.3 | Dropout                 | 0     \n",
      "41 | backbone.body.2.residualblock.doubleconv2d.4 | Conv2d                  | 36.9 K\n",
      "42 | backbone.body.2.residualblock.doubleconv2d.5 | BatchNorm2d             | 128   \n",
      "43 | backbone.body.2.residualblock.doubleconv2d.6 | PReLU                   | 1     \n",
      "44 | backbone.body.2.residualblock.doubleconv2d.7 | Dropout                 | 0     \n",
      "45 | backbone.body.3                              | ResidualBlock           | 74.0 K\n",
      "46 | backbone.body.3.residualblock                | DoubleConv2d            | 74.0 K\n",
      "47 | backbone.body.3.residualblock.doubleconv2d   | Sequential              | 74.0 K\n",
      "48 | backbone.body.3.residualblock.doubleconv2d.0 | Conv2d                  | 36.9 K\n",
      "49 | backbone.body.3.residualblock.doubleconv2d.1 | BatchNorm2d             | 128   \n",
      "50 | backbone.body.3.residualblock.doubleconv2d.2 | PReLU                   | 1     \n",
      "51 | backbone.body.3.residualblock.doubleconv2d.3 | Dropout                 | 0     \n",
      "52 | backbone.body.3.residualblock.doubleconv2d.4 | Conv2d                  | 36.9 K\n",
      "53 | backbone.body.3.residualblock.doubleconv2d.5 | BatchNorm2d             | 128   \n",
      "54 | backbone.body.3.residualblock.doubleconv2d.6 | PReLU                   | 1     \n",
      "55 | backbone.body.3.residualblock.doubleconv2d.7 | Dropout                 | 0     \n",
      "56 | backbone.sr                                  | PixelShuffleBlock       | 44    \n",
      "57 | backbone.sr.upsample                         | Sequential              | 44    \n",
      "58 | backbone.sr.upsample.0                       | PixelShuffle            | 0     \n",
      "59 | backbone.sr.upsample.1                       | Conv2d                  | 16    \n",
      "60 | backbone.sr.upsample.2                       | BatchNorm2d             | 8     \n",
      "61 | backbone.sr.upsample.3                       | PReLU                   | 1     \n",
      "62 | backbone.sr.upsample.4                       | Dropout                 | 0     \n",
      "63 | backbone.sr.upsample.5                       | Conv2d                  | 12    \n",
      "64 | backbone.sr.upsample.6                       | BatchNorm2d             | 6     \n",
      "65 | backbone.sr.upsample.7                       | PReLU                   | 1     \n",
      "66 | backbone.resize                              | Resize                  | 0     \n",
      "67 | baseline                                     | BicubicUpscaledBaseline | 0     \n",
      "68 | baseline.resize                              | Resize                  | 0     \n",
      "------------------------------------------------------------------------------------------\n",
      "334 K     Trainable params\n",
      "0         Non-trainable params\n",
      "334 K     Total params\n",
      "1.339     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Model Backbone Architecture =====\n",
      "SRCNN(\n",
      "  (encoder): DoubleConv2d(\n",
      "    (doubleconv2d): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False, padding_mode=reflect)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): PReLU(num_parameters=1)\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False, padding_mode=reflect)\n",
      "      (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): PReLU(num_parameters=1)\n",
      "      (7): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (body): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (residualblock): DoubleConv2d(\n",
      "        (doubleconv2d): Sequential(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False, padding_mode=reflect)\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): PReLU(num_parameters=1)\n",
      "          (3): Dropout(p=0.5, inplace=False)\n",
      "          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False, padding_mode=reflect)\n",
      "          (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (6): PReLU(num_parameters=1)\n",
      "          (7): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (residualblock): DoubleConv2d(\n",
      "        (doubleconv2d): Sequential(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False, padding_mode=reflect)\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): PReLU(num_parameters=1)\n",
      "          (3): Dropout(p=0.5, inplace=False)\n",
      "          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False, padding_mode=reflect)\n",
      "          (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (6): PReLU(num_parameters=1)\n",
      "          (7): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (residualblock): DoubleConv2d(\n",
      "        (doubleconv2d): Sequential(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False, padding_mode=reflect)\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): PReLU(num_parameters=1)\n",
      "          (3): Dropout(p=0.5, inplace=False)\n",
      "          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False, padding_mode=reflect)\n",
      "          (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (6): PReLU(num_parameters=1)\n",
      "          (7): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): ResidualBlock(\n",
      "      (residualblock): DoubleConv2d(\n",
      "        (doubleconv2d): Sequential(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False, padding_mode=reflect)\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): PReLU(num_parameters=1)\n",
      "          (3): Dropout(p=0.5, inplace=False)\n",
      "          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False, padding_mode=reflect)\n",
      "          (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (6): PReLU(num_parameters=1)\n",
      "          (7): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (sr): PixelShuffleBlock(\n",
      "    (upsample): Sequential(\n",
      "      (0): PixelShuffle(upscale_factor=4)\n",
      "      (1): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False, padding_mode=reflect)\n",
      "      (2): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): PReLU(num_parameters=1)\n",
      "      (4): Dropout(p=0.5, inplace=False)\n",
      "      (5): Conv2d(4, 3, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False, padding_mode=reflect)\n",
      "      (6): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (7): PReLU(num_parameters=1)\n",
      "    )\n",
      "  )\n",
      "  (resize): Resize()\n",
      ")\n",
      "======================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c9171e69bf4d9882dbd5167e8dcbd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b0868a2759142ec86cd299d30432eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739c1b08533343eabef550163118253f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_training_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdefault_train_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_on_windows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\n",
      "Cell \u001b[0;32mIn[2], line 43\u001b[0m, in \u001b[0;36mrun_training_command\u001b[0;34m(training_command, running_on_windows)\u001b[0m\n",
      "\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m running_on_windows:\n",
      "\u001b[1;32m     42\u001b[0m     sys\u001b[38;5;241m.\u001b[39margv \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--num_workers\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;66;03m# 윈도우에서는 멀티프로세싱 비효율(than linux)\u001b[39;00m\n",
      "\u001b[0;32m---> 43\u001b[0m \u001b[43mcli_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Develop/aiffel/zee/ZEE_AID_data/src/train.py:48\u001b[0m, in \u001b[0;36mcli_main\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m======================================\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m     47\u001b[0m add_callbacks(args, dataloaders)\n",
      "\u001b[0;32m---> 48\u001b[0m \u001b[43mgenerate_and_run_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     49\u001b[0m finish_wandb_logging(args)\n",
      "\n",
      "File \u001b[0;32m~/Develop/aiffel/zee/ZEE_AID_data/src/train.py:112\u001b[0m, in \u001b[0;36mgenerate_and_run_trainer\u001b[0;34m(args, dataloaders, model)\u001b[0m\n",
      "\u001b[1;32m    109\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(model, dataloaders[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m], dataloaders[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args\u001b[38;5;241m.\u001b[39mfast_dev_run:\n",
      "\u001b[0;32m--> 112\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:794\u001b[0m, in \u001b[0;36mTrainer.test\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n",
      "\u001b[1;32m    792\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_unwrap_optimized(model)\n",
      "\u001b[1;32m    793\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n",
      "\u001b[0;32m--> 794\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_test_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\n",
      "\u001b[1;32m    796\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m     36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "\u001b[1;32m     41\u001b[0m     trainer\u001b[38;5;241m.\u001b[39m_call_teardown_hook()\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:842\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n",
      "\u001b[1;32m    839\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tested_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt_path  \u001b[38;5;66;03m# TODO: remove in v1.8\u001b[39;00m\n",
      "\u001b[1;32m    841\u001b[0m \u001b[38;5;66;03m# run test\u001b[39;00m\n",
      "\u001b[0;32m--> 842\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "\u001b[1;32m    845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1112\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n",
      "\u001b[1;32m   1108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mrestore_training_state()\n",
      "\u001b[1;32m   1110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n",
      "\u001b[0;32m-> 1112\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1114\u001b[0m log\u001b[38;5;241m.\u001b[39mdetail(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m   1115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown()\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1188\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m   1185\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mdispatch(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluating:\n",
      "\u001b[0;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n",
      "\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1228\u001b[0m, in \u001b[0;36mTrainer._run_evaluate\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m   1223\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluation_loop\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;32m   1225\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m), _evaluation_context(\n",
      "\u001b[1;32m   1226\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_mode\n",
      "\u001b[1;32m   1227\u001b[0m ):\n",
      "\u001b[0;32m-> 1228\u001b[0m     eval_loop_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1230\u001b[0m \u001b[38;5;66;03m# remove the tensors from the eval results\u001b[39;00m\n",
      "\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m eval_loop_results:\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:152\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_dataloaders \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;32m    151\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataloader_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dataloader_idx\n",
      "\u001b[0;32m--> 152\u001b[0m dl_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_max_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# store batch level output per dataloader\u001b[39;00m\n",
      "\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs\u001b[38;5;241m.\u001b[39mappend(dl_outputs)\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:121\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n",
      "\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_fetcher, DataLoaderIterDataFetcher):\n",
      "\u001b[1;32m    120\u001b[0m     batch_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mcurrent\u001b[38;5;241m.\u001b[39mready\n",
      "\u001b[0;32m--> 121\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    123\u001b[0m     batch_idx, batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(data_fetcher)\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py:184\u001b[0m, in \u001b[0;36mAbstractDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetching_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py:265\u001b[0m, in \u001b[0;36mDataFetcher.fetching_function\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n",
      "\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n",
      "\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 265\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch_next_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    266\u001b[0m         \u001b[38;5;66;03m# consume the batch we just fetched\u001b[39;00m\n",
      "\u001b[1;32m    267\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatches\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/site-packages/pytorch_lightning/utilities/fetching.py:280\u001b[0m, in \u001b[0;36mDataFetcher._fetch_next_batch\u001b[0;34m(self, iterator)\u001b[0m\n",
      "\u001b[1;32m    278\u001b[0m start_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_fetch_start()\n",
      "\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 280\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop_profiler()\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n",
      "\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n",
      "\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n",
      "\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n",
      "\u001b[1;32m    707\u001b[0m ):\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n",
      "\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n",
      "\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n",
      "\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n",
      "\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n",
      "\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n",
      "\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n",
      "\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/site-packages/torch/utils/data/dataset.py:350\u001b[0m, in \u001b[0;36mConcatDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n",
      "\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m    349\u001b[0m     sample_idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcumulative_sizes[dataset_idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;32m--> 350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_idx\u001b[49m\u001b[43m]\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Develop/aiffel/zee/ZEE_AID_data/src/datasets.py:123\u001b[0m, in \u001b[0;36mTransformDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n",
      "\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx: \u001b[38;5;28mint\u001b[39m):\n",
      "\u001b[1;32m    114\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m    데이터셋에서 이미지를 가져오고 변환을 적용하는 함수\u001b[39;00m\n",
      "\u001b[1;32m    116\u001b[0m \n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m        Tensor: 변환이 적용된 이미지\u001b[39;00m\n",
      "\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;32m--> 123\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/site-packages/torch/utils/data/dataset.py:412\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n",
      "\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n",
      "\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n",
      "\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Develop/aiffel/zee/ZEE_AID_data/src/datasets.py:158\u001b[0m, in \u001b[0;36mDictDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n",
      "\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n",
      "\u001b[1;32m    149\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m    데이터셋에서 이미지를 가져오는 함수\u001b[39;00m\n",
      "\u001b[1;32m    151\u001b[0m \n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m        dict: 각 데이터셋에서 가져온 이미지\u001b[39;00m\n",
      "\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {dataset_type: dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m dataset_type, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\n",
      "File \u001b[0;32m~/Develop/aiffel/zee/ZEE_AID_data/src/datasets.py:158\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n",
      "\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n",
      "\u001b[1;32m    149\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m    데이터셋에서 이미지를 가져오는 함수\u001b[39;00m\n",
      "\u001b[1;32m    151\u001b[0m \n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m        dict: 각 데이터셋에서 가져온 이미지\u001b[39;00m\n",
      "\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {dataset_type: \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m dataset_type, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\n",
      "File \u001b[0;32m~/Develop/aiffel/zee/ZEE_AID_data/src/datasets.py:63\u001b[0m, in \u001b[0;36mAidDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n",
      "\u001b[1;32m     60\u001b[0m     img \u001b[38;5;241m=\u001b[39m read_image(path)\n",
      "\u001b[1;32m     61\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n",
      "\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# 이미 캐시에 있는 경우 캐시에서 반환\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/Develop/aiffel/zee/ZEE_AID_data/src/datasets.py:94\u001b[0m, in \u001b[0;36mAidDataset.cache_img\u001b[0;34m(self, idx, x)\u001b[0m\n",
      "\u001b[1;32m     86\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m이미지를 캐시에 저장하는 함수\u001b[39;00m\n",
      "\u001b[1;32m     88\u001b[0m \n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m    x (Tensor): 저장할 이미지\u001b[39;00m\n",
      "\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_cache:\n",
      "\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m x\n",
      "\n",
      "File \u001b[0;32m<string>:2\u001b[0m, in \u001b[0;36m__setitem__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/multiprocessing/managers.py:809\u001b[0m, in \u001b[0;36mBaseProxy._callmethod\u001b[0;34m(self, methodname, args, kwds)\u001b[0m\n",
      "\u001b[1;32m    806\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect()\n",
      "\u001b[1;32m    807\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tls\u001b[38;5;241m.\u001b[39mconnection\n",
      "\u001b[0;32m--> 809\u001b[0m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethodname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    810\u001b[0m kind, result \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mrecv()\n",
      "\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#RETURN\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/multiprocessing/connection.py:206\u001b[0m, in \u001b[0;36m_ConnectionBase.send\u001b[0;34m(self, obj)\u001b[0m\n",
      "\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n",
      "\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_writable()\n",
      "\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_bytes(\u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/multiprocessing/reduction.py:51\u001b[0m, in \u001b[0;36mForkingPickler.dumps\u001b[0;34m(cls, obj, protocol)\u001b[0m\n",
      "\u001b[1;32m     48\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n",
      "\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdumps\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[1;32m     50\u001b[0m     buf \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO()\n",
      "\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buf\u001b[38;5;241m.\u001b[39mgetbuffer()\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/site-packages/torch/multiprocessing/reductions.py:607\u001b[0m, in \u001b[0;36mreduce_storage\u001b[0;34m(storage)\u001b[0m\n",
      "\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n",
      "\u001b[1;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot pickle meta storage; try pickling a meta tensor instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m    605\u001b[0m     )\n",
      "\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m get_sharing_strategy() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_system\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;32m--> 607\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_share_filename_cpu_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    608\u001b[0m     cache_key \u001b[38;5;241m=\u001b[39m metadata[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;32m    609\u001b[0m     rebuild \u001b[38;5;241m=\u001b[39m rebuild_storage_filename\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/site-packages/torch/storage.py:437\u001b[0m, in \u001b[0;36m_share_memory_lock_protected.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    434\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;32m    439\u001b[0m     \u001b[38;5;66;03m# If we acquired the storage lock here and we're done working on it\u001b[39;00m\n",
      "\u001b[1;32m    440\u001b[0m     \u001b[38;5;66;03m# we can now release it and free the entry.\u001b[39;00m\n",
      "\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m to_free \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m    442\u001b[0m         \u001b[38;5;66;03m# Ensure that the cdata from the storage didn't change and only\u001b[39;00m\n",
      "\u001b[1;32m    443\u001b[0m         \u001b[38;5;66;03m# the data_ptr did.\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/site-packages/torch/storage.py:516\u001b[0m, in \u001b[0;36mUntypedStorage._share_filename_cpu_\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    514\u001b[0m \u001b[38;5;129m@_share_memory_lock_protected\u001b[39m\n",
      "\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_share_filename_cpu_\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "\u001b[0;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_share_filename_cpu_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Broken pipe"
     ]
    }
   ],
   "source": [
    "run_training_commanㅇㅇd(default_train_command, running_on_windows=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_lr dataset path: AID-dataset/train/LR_x2\n",
      "train_hr dataset path: AID-dataset/train/HR\n",
      "val_lr dataset path: AID-dataset/val/LR_x2\n",
      "val_hr dataset path: AID-dataset/val/HR\n",
      "test_lr dataset path: AID-dataset/test/LR_x2\n",
      "test_hr dataset path: AID-dataset/test/HR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/worldstrat/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:263: UserWarning: Attribute 'backbone' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['backbone'])`.\n",
      "  rank_zero_warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `wandb.require('service')` is a no-op as it is now the default behavior.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling the dataset splits using 42\n",
      "Shuffling the dataset splits using 42\n",
      "Shuffling the dataset splits using 42\n",
      "Shuffling the dataset splits using 42\n",
      "Train set size: 4710\n",
      "Val set size: 150\n",
      "Test set size: 2000\n",
      "\n",
      "===== Model Backbone Architecture =====\n",
      "SRCNN(\n",
      "  (encoder): DoubleConv2d(\n",
      "    (doubleconv2d): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False, padding_mode=reflect)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): PReLU(num_parameters=1)\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "      (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False, padding_mode=reflect)\n",
      "      (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): PReLU(num_parameters=1)\n",
      "      (7): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (body): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (residualblock): DoubleConv2d(\n",
      "        (doubleconv2d): Sequential(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False, padding_mode=reflect)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): PReLU(num_parameters=1)\n",
      "          (3): Dropout(p=0.5, inplace=False)\n",
      "          (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same, bias=False, padding_mode=reflect)\n",
      "          (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (6): PReLU(num_parameters=1)\n",
      "          (7): Dropout(p=0.5, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (sr): PixelShuffleBlock(\n",
      "    (upsample): Sequential(\n",
      "      (0): PixelShuffle(upscale_factor=2)\n",
      "      (1): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False, padding_mode=reflect)\n",
      "      (2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): PReLU(num_parameters=1)\n",
      "      (4): Dropout(p=0.5, inplace=False)\n",
      "      (5): Conv2d(8, 3, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False, padding_mode=reflect)\n",
      "      (6): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (7): PReLU(num_parameters=1)\n",
      "    )\n",
      "  )\n",
      "  (resize): Resize()\n",
      ")\n",
      "======================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_training_command(default_train_command, running_on_windows=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "worldstrat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "baca3009b4d682dcab5112255c43ce6cf840011b57f9a9f1cb26993b30d438e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
